% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{Vaswani2017}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~Gomez, L.~Kaiser,
  and I.~Polosukhin, ``Attention is all you need. arxiv 2017,'' \emph{arXiv
  preprint arXiv:1706.03762}, 2017.

\bibitem{Dai2015}
A.~M. Dai and Q.~V. Le, ``Semi-supervised sequence learning,'' in \emph{NIPS},
  2015.

\bibitem{Radford2018}
A.~Radford, K.~Narasimhan, T.~Salimans, and I.~Sutskever, ``Improving language
  understanding with unsupervised learning,'' \emph{Technical report, OpenAI},
  2018.

\bibitem{Devlin2018}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova, ``Bert: Pre-training of deep
  bidirectional transformers for language understanding,'' \emph{arXiv preprint
  arXiv:1810.04805}, 2018.

\bibitem{Yang2019}
Z.~Yang, Z.~Dai, Y.~Yang, J.~Carbonell, R.~R. Salakhutdinov, and Q.~V. Le,
  ``Xlnet: Generalized autoregressive pretraining for language understanding,''
  in \emph{Advances in neural information processing systems}, 2019, pp.
  5753--5763.

\bibitem{Sanh2019}
V.~Sanh, L.~Debut, J.~Chaumond, and T.~Wolf, ``Distilbert, a distilled version
  of bert: smaller, faster, cheaper and lighter,'' \emph{arXiv preprint
  arXiv:1910.01108}, 2019.

\bibitem{Liu2019}
Y.~Liu, M.~Ott, N.~Goyal, J.~Du, M.~Joshi, D.~Chen, O.~Levy, M.~Lewis,
  L.~Zettlemoyer, and V.~Stoyanov, ``Roberta: A robustly optimized bert
  pretraining approach,'' \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem{Conneau2019}
A.~Conneau, K.~Khandelwal, N.~Goyal, V.~Chaudhary, G.~Wenzek, F.~Guzm{\'a}n,
  E.~Grave, M.~Ott, L.~Zettlemoyer, and V.~Stoyanov, ``Unsupervised
  cross-lingual representation learning at scale,'' \emph{arXiv preprint
  arXiv:1911.02116}, 2019.

\bibitem{Radford2019}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, and I.~Sutskever, ``Language
  models are unsupervised multitask learners,'' \emph{OpenAI Blog}, vol.~1,
  no.~8, p.~9, 2019.

\bibitem{Lan2019}
Z.~Lan, M.~Chen, S.~Goodman, K.~Gimpel, P.~Sharma, and R.~Soricut, ``Albert: A
  lite bert for self-supervised learning of language representations,''
  \emph{arXiv preprint arXiv:1909.11942}, 2019.

\bibitem{Brown2020}
T.~B. Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell \emph{et~al.}, ``Language
  models are few-shot learners,'' \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem{Nguyen2020}
D.~Q. Nguyen and A.~T. Nguyen, ``Phobert: Pre-trained language models for
  vietnamese,'' \emph{arXiv preprint arXiv:2003.00744}, 2020.

\bibitem{Taylor1953}
W.~L. Taylor, ``“cloze procedure”: A new tool for measuring readability,''
  \emph{Journalism quarterly}, vol.~30, no.~4, pp. 415--433, 1953.

\bibitem{You2019}
Y.~You, J.~Li, S.~Reddi, J.~Hseu, S.~Kumar, S.~Bhojanapalli, X.~Song,
  J.~Demmel, K.~Keutzer, and C.-J. Hsieh, ``Large batch optimization for deep
  learning: Training bert in 76 minutes,'' \emph{arXiv preprint
  arXiv:1904.00962}, 2019.

\bibitem{Sennrich2015}
R.~Sennrich, B.~Haddow, and A.~Birch, ``Neural machine translation of rare
  words with subword units,'' \emph{arXiv preprint arXiv:1508.07909}, 2015.

\end{thebibliography}
